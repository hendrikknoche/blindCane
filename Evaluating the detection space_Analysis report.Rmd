---
title: "Evaluating the detection space_Analysis report"
author: "Milo Marsfeldt Skovfoged & Alexander Schiller Rasmussen"
date: "25/5/2020"
output:
  word_document: default
  pdf_document: default
---

1. [Introduction](#introduction)
2. [Data information](#data-information)
3. [Predicition of course completion time](#predicition-of-course-completion-time)

4. [Number of collisions](#number-of-collisions)
5. [Walking speed](#walking-speed)
6. [Number of detections](#number-of-detections)

7. [Analysis conclusion](#analysis-conclusion)



## Introduction
This is the statistical report associated with the paper "Evaluating the detection space" by Milo Marsfeldt Skovfoged & Alexander Schiller Rasmussen. The field of study lies in researching visually impaired/blind navigation through environments, to find an ideal Range of looking ahead and Field of Detection (FOD), for the most progressive travel-route, while avoiding collisions as much as possible.
This report gives an overview of what data was gathered and analysed, in regrads to different points of interrest.

```{r setup, include=FALSE}
library(tidyverse)
library(readbulk)
library(lubridate)
library(ggplot2)
library(scales) 
library(magrittr)
library(knitr)
library("car")
library(rmarkdown)
#GetData
load('data_all.rda')

#Data Grouped by Snario
daggByScen <- df %>% 
  filter(Person_Speed<3) %>%
  group_by(testID,day,Scenario,FOD,Range) %>%
  summarize(avgSpeed=mean(Person_Speed),medianSpeed=median(Person_Speed),maxSpeed=max(Person_Speed),minSpeed=min(Person_Speed),objectDetected=sum(objDet,na.rm = TRUE),objectCollisions=sum(objColl,na.rm = TRUE),Time=max(Time_in_MS*1000)) %>% 
  arrange(testID)

#add Coloum with sum of total time spent 
daggByScen$totalTimeTraining<-round(cumsum(daggByScen$Time))

#add Coloum with total time spent for a given FOD with a given Range
daggByScen <- daggByScen %>%
  group_by(FOD,day,Range) %>%
  mutate(timeFDRtrain=round(cumsum(Time)))

#add Coloum with total time spent for a given FOD
daggByScen <- daggByScen%>%group_by(FOD,day)%>%mutate(timeFDtrain=round(cumsum(Time)))

#add Coloum with total time spent for a given Day
daggByScen <- daggByScen%>%group_by(day)%>%mutate(timeDtrain=round(cumsum(Time)),totalTimeTrainingHrs=totalTimeTraining/3600)
daggByCol <- df %>% filter(Person_Speed<3)%>%group_by(testID,day,Scenario,FOD,Range,objColl)%>%summarize(avgSpeed=mean(Person_Speed),medianSpeed=median(Person_Speed),maxSpeed=max(Person_Speed),minSpeed=min(Person_Speed),objectDetected=sum(objDet,na.rm = TRUE),objectCollisions=sum(objColl,na.rm = TRUE),Time=max(Time_in_MS*1000))%>% arrange(testID)


#Model equation
model_equation <- function(model, ...) {
  format_args <- list(...)
  
  model_coeff <- model$coefficients
  format_args$x <- abs(model$coefficients)
  model_coeff_sign <- sign(model_coeff)
  model_coeff_prefix <- case_when(model_coeff_sign == -1 ~ " - ",
                                  model_coeff_sign == 1 ~ " + ",
                                  model_coeff_sign == 0 ~ " + ")
  model_eqn <- paste(strsplit(as.character(model$call$formula), "~")[[2]], # 'y'
                     "=",
                     paste(if_else(model_coeff[1]<0, "- ", ""),
                           do.call(format, format_args)[1],
                           paste(model_coeff_prefix[-1],
                                 do.call(format, format_args)[-1],
                                 " * ",
                                 names(model_coeff[-1]),
                                 sep = "", collapse = ""),
                           sep = ""))
  return(model_eqn)
}


daggHeat <- df %>% filter(Person_Speed<3)%>%group_by(Scenario,FOD,Range,day)%>%summarize(avgSpeed=mean(Person_Speed),medianSpeed=median(Person_Speed),maxSpeed=max(Person_Speed),minSpeed=min(Person_Speed),objectDetected=sum(objDet,na.rm = TRUE),objectCollisions=sum(objColl,na.rm = TRUE),Time=max(Time_in_MS*1000))

###----------------Numeric Analysis part----------------###


dataWhole <- daggByScen %>%
  group_by(FOD, Range) %>%
  summarize(objectCollisions = mean(objectCollisions, na.rm = TRUE), objectDetected = mean(objectDetected, na.rm = TRUE), Time = mean(Time * 1000))

test<-daggByScen %>%
  group_by(FOD, Range) %>%
  summarize(objectCollisions = mean(objectCollisions, na.rm = TRUE), objectDetected = mean(objectDetected, na.rm = TRUE), Time = mean(Time * 1000))


#WOBBL removes the baseline from the data and leaves us with Whole-room and Corridor
daggByScenWOBL<-daggByScen[!daggByScen$FOD=="Baseline",]
daggByScenWOBLC<-daggByScen[!daggByScen$FOD=="Corridor",]
daggByScenWOBLW<-daggByScen[!daggByScen$FOD=="WholeRoom",]
m<-lm(Time~FOD*Range+totalTimeTraining,data=daggByScen[!daggByScen$FOD=="Baseline",])
summary(m)
model_equation(m)
daggByScenWOBL$predTime<-predict(m)

m0<-lm(log(Time)~log(totalTimeTraining)+FOD*log(Range),data=daggByScen)


baseDat <- daggByScen[daggByScen$FOD=="Baseline",]
mbaseDat <- lm(log(Time)~log(totalTimeTraining),data=baseDat)


wrDat <- daggByScen[daggByScen$FOD=="WholeRoom",]
mwrDat<- lm(log(Time)~log(totalTimeTraining),data=wrDat)
wr2Dat <- wrDat[wrDat$Range=="2",]
wr3Dat <- wrDat[wrDat$Range=="3",]
wr4Dat <- wrDat[wrDat$Range=="4",]

corrDat <- daggByScen[daggByScen$FOD=="Corridor",]
mcorrDat<- lm(log(Time)~log(totalTimeTraining),data=corrDat)
corr2Dat <- corrDat[corrDat$Range=="2",]
corr3Dat <- corrDat[corrDat$Range=="3",]
corr4Dat <- corrDat[corrDat$Range=="4",]

###----------------Plots data----------------###
lower_ci <- function(mean, se, n, conf_level = 0.95){
  lower_ci <- mean - qt(1 - ((1 - conf_level) / 2), n - 1) * se
}
upper_ci <- function(mean, se, n, conf_level = 0.95){
  upper_ci <- mean + qt(1 - ((1 - conf_level) / 2), n - 1) * se
}

daggByDFR<-daggByScen %>%group_by(FOD,day,Range)%>%summarize(totalTimeTraining=max(totalTimeTraining),avgTime=mean(Time),ssd = sd(Time),count=n(),se = ssd / sqrt(count))%>%mutate(lower_ci=lower_ci(avgTime,se,count),upper_ci=upper_ci(avgTime,se,count))

#Learning curve
totalTimeTraining=1:6000
#Set to summary variables 
FOD = "Baseline"
Range="1"
b=21.45979
z= -0.06329
#This is the Time fed into dxf
Time<- b*totalTimeTraining^z
baseDatxf=as.data.frame(totalTimeTraining)
baseDatxf=cbind(baseDatxf,Time)
baseDatxf=cbind(baseDatxf,FOD)
baseDatxf=cbind(baseDatxf,Range)

FOD = "Corridor" 
Range="2"
b=31.32990
z=-0.10500
Time<- b*totalTimeTraining^z
corr2Datxf=as.data.frame(totalTimeTraining)
corr2Datxf=cbind(corr2Datxf,Time)
corr2Datxf=cbind(corr2Datxf,FOD)
corr2Datxf=cbind(corr2Datxf,Range)

Range="3"
b=54.72364
z=-0.17482
Time<- b*totalTimeTraining^z
corr3Datxf=as.data.frame(totalTimeTraining)
corr3Datxf=cbind(corr3Datxf,Time)
corr3Datxf=cbind(corr3Datxf,FOD)
corr3Datxf=cbind(corr3Datxf,Range)

Range="4"
b=9.81972
z=0.03528
Time<- b*totalTimeTraining^z
corr4Datxf=as.data.frame(totalTimeTraining)
corr4Datxf=cbind(corr4Datxf,Time)
corr4Datxf=cbind(corr4Datxf,FOD)
corr4Datxf=cbind(corr4Datxf,Range)


FOD = "WholeRoom"
Range="2"
b=37.2835	
z=-0.1273
Time<- b*totalTimeTraining^z
wr2Datxf=as.data.frame(totalTimeTraining)
wr2Datxf=cbind(wr2Datxf,Time)
wr2Datxf=cbind(wr2Datxf,FOD)
wr2Datxf=cbind(wr2Datxf,Range)

Range="3"
b=47.50384	
z=-0.15325
Time<- b*totalTimeTraining^z
wr3Datxf=as.data.frame(totalTimeTraining)
wr3Datxf=cbind(wr3Datxf,Time)
wr3Datxf=cbind(wr3Datxf,FOD)
wr3Datxf=cbind(wr3Datxf,Range)

Range="4"
b=54.86090	
z=-0.16613
Time<- b*totalTimeTraining^z
wr4Datxf=as.data.frame(totalTimeTraining)
wr4Datxf=cbind(wr4Datxf,Time)
wr4Datxf=cbind(wr4Datxf,FOD)
wr4Datxf=cbind(wr4Datxf,Range)

knitr::opts_chunk$set(echo = TRUE)
```

## Data information
Below, a summary of our data is presented. In total, 420 tests were completed over three days (140 per day), using three different Field Of Detections (FOD - Baseline, WholeRoom and Corridor), with exception of WholeRoom and Corridor differing between three ranges (two, three and four meters), as Baseline represents the original wite cane. Scenarios describe the amount of different parkours the system was tested on (each condition with different lengths). In addition, each test logged the speed of the participant, the amount of objects detected by the cane, the amount of collisions by the user and the completion time of the individual parkours.
```{r first summary, echo=FALSE}

summary(daggByScen)

#kable(summary(daggByScen[1:16]), caption = "Data")
# summary(nameOfDataFile)
#Shows a table of the data (e.g. could be the coefficients for completion time to FOD compairson)
```

To test for normality in our data, we conducted a Shapiro Wilks test

```{r Normality check, echo=FALSE}
shapiro.test(daggByScen$avgSpeed)

shapiro.test(daggByScen$objectDetected)

shapiro.test(daggByScen$objectCollisions)
```

and illustrated the data in qq-plots for a better overview.

```{r Normality plots, echo=FALSE}
qqPlot(daggByScen$avgSpeed)
qqPlot(daggByScen$objectDetected)
qqPlot(daggByScen$objectCollisions)
qqPlot(daggByScen$Time)
```

The p-values show a significant difference and, thereby rejects the nullhypothesis of the data following a normal distributed. For analysing the results further we will be using glm models wiht a Poisson distribution.


<<<<<<< HEAD
To visualise the difference between FOD and range, in regards to object collisions, object detections and walking speed, the following models were summarized, using Guassian and Possoin distirbutions, which regardless of our data's distribution did not change the outcome of significance. 

=======
### Training Time
###### Walking speed
```{r Training time Walking Speed., echo=FALSE}
summary(lm(avgSpeed ~ totalTimeTrainingHrs, data=daggByScen))
```

##### Object Detected
```{r Training time Detections, echo=FALSE}
summary(glm(objectDetected ~ totalTimeTrainingHrs, family="poisson", data=daggByScen))
```

##### Object Collisions
```{r Training time Collisions, echo=FALSE}
summary(glm(objectCollisions ~ totalTimeTrainingHrs, family="poisson", data=daggByScen))
```


### Walking speed
###### Wholeroom
```{r Normal aEMA Spd., echo=FALSE}
summary(lm(avgSpeed ~ Range+totalTimeTrainingHrs, data=wrDat))
```

###### Corridor
```{r Body-pr. aEMA Spd., echo=FALSE}
summary(lm(avgSpeed ~ Range+totalTimeTrainingHrs,data=corrDat))
```

###### AllData
```{r Baseline/aEMA Spd., echo=FALSE}
summary(lm(avgSpeed ~ FOD + totalTimeTrainingHrs, data=daggByScen))
```

###### FOD and Range
```{r Speed based on FOD and Range, echo=FALSE}
summary(lm(avgSpeed ~ FOD*Range + totalTimeTrainingHrs, data=daggByScen))
```

######Detections
```{r Speed based on Detections, echo=FALSE}
summary(lm(avgSpeed ~ objectDetected+totalTimeTrainingHrs,data=corrDat))
```

######Collisions
```{r Speed based on collisions, echo=FALSE}
summary(lm(avgSpeed ~ objectCollisions+totalTimeTrainingHrs,data=corrDat))
```

### Object detections

###### Wholeroom
```{r Normal aEMA Det., echo=FALSE}
summary(glm(objectDetected ~ Range + totalTimeTrainingHrs, family="poisson", data=wrDat))
```

###### Corridor
```{r Body-pr. aEMA Det., echo=FALSE}
summary(glm(objectDetected ~ Range+totalTimeTrainingHrs, family="poisson",data=corrDat))
```


###### AllData
```{r Baseline/aEMA Det., echo=FALSE}
summary(glm(objectDetected ~ FOD + totalTimeTrainingHrs, family="poisson",data=daggByScen))
```

###### Detections based on FOD and Range
```{r Detections based on FOD and Range, echo=FALSE}
summary(glm(objectDetected ~ FOD *Range + totalTimeTrainingHrs, family="poisson", data=daggByScen))
```

###### Detections based on Collisions
```{r Detections based on Collisions, echo=FALSE}
summary(glm(objectDetected ~ objectCollisions + totalTimeTrainingHrs, family="poisson",data=daggByScen))
```

###### Detections based on Time
```{r Detections based on Time, echo=FALSE}
summary(glm(objectDetected ~ Time + totalTimeTrainingHrs, family="poisson",data=daggByScen))
```


### Object collisions

###### Wholeroom
```{r Normal aEMA Coll., echo=FALSE}
summary(glm(objectCollisions ~ Range+totalTimeTrainingHrs, family="poisson",data=wrDat))
```

###### Corridor 
```{r Body-pr. aEMA Coll., echo=FALSE}
summary(glm(objectCollisions ~ Range+totalTimeTrainingHrs ,family="poisson",data=corrDat))
```


###### Alldata
```{r Baseline/aEMA Coll., echo=FALSE}
summary(glm(objectCollisions ~ FOD + totalTimeTrainingHrs ,family="poisson",data=daggByScen))
```


## Number of collisions
The plot below summarizes the number of collisions for the baseline and the two conditions at each range. Increasing the range from 1 - 2-m results in a 33\% and 40\% decrease in collisions for the \textit{Body-preview-} and \textit{Regular  aEMA} respectably. For ranges over two meter both aEMA have a non-significant increase in collisions - \textit{Regular aEMA} ($\beta$ = 0.052, z(177) = 0.578, p = .6) and \textit{Body-preview aEMA} ($\beta$ = 0.09, z(177) = 1.04, p = .3). 

```{r CollisionsPlot, echo=FALSE}
daggColl <- daggByScen %>%
  group_by(Range, FOD)%>%
  summarise(avgColl=mean(objectCollisions),
            smean = mean(objectCollisions, na.rm = TRUE),
            ssd = sd(objectCollisions, na.rm = TRUE),
            count = n()) %>%
  mutate(
    se = ssd / sqrt(count),
    lowerci = lower_ci(smean, se, count),
    upperci = upper_ci(smean, se, count))

ggplot(data = daggColl, aes(x=Range, y=avgColl, group=FOD, color=FOD))+
  geom_point(position = position_dodge(0.1), alpha=1)+
  geom_line(position = position_dodge(0.1), alpha=1, size=1)+
  #geom_bar(position="dodge", stat = "identity", size=.3)+
  geom_errorbar(aes(ymin = lowerci, ymax = upperci), width = 0.2, color = "Black", position = position_dodge(0.1)) +
  geom_text(aes(label = round(avgColl, 1)), size = 6, alpha=1, position = position_dodge(0.4), vjust = -0.5) +
  scale_fill_hue(name="Condition", labels=c("White Cane", "Body-preview aEMA", "Normal aEMA"))+
  ggtitle("Number of Objects Collisions per Range and Condition")+
  ylab("Mean Number of Collisions") +
  scale_y_continuous()+
  theme_bw()
  
```

 
## Walking speed
The mean walking speed for the baseline and the two conditions at each range can be seen in the plot below. Multiple regression analysis showed that for the \textit{Regular aEMA} increasing the detection range resulted in decreasing the participants walking speed ($\beta$ = -0.019, t(177) = -3.13, p = .002), however, using the \textit{Body-preview aEMA} had a non-significant increase in waking speed ($\beta$ = 0.014, t(177) = 1.85, p = .06). When comparing the two conditions to the white cane showed that the \textit{Body-preview aEMA} did not significant influence the walking speed of the user ($\beta$ < 0.001, t(416) = 0.033, p = 0.9), while the \textit{Regular aEMA} resulted the user walking slower ($\beta$ = -0.05, t(416) = -4.22, p < .001).

The more obstacles the participant detected the slower he walked ($\beta$ = -0.01, z(177) = -8.9, p < .001). Likewise, colliding with an obstacle reduces the participants walking speed ($\beta$ = -0.03, z(177) = -4.58, p < .001).

```{r SpeedPlot, echo=FALSE}
daggSpeed <- daggByScen %>%
  group_by(Range, FOD)%>%
  summarise(newAvgSpeed=mean(avgSpeed),
            smean = mean(avgSpeed, na.rm = TRUE),
            ssd = sd(avgSpeed, na.rm = TRUE),
            count = n()) %>%
  mutate(
    se = ssd / sqrt(count),
    lowerci = lower_ci(smean, se, count),
    upperci = upper_ci(smean, se, count))

ggplot(data = daggSpeed, aes(x=Range, y=newAvgSpeed, group=FOD, color=FOD))+
  geom_point(position = position_dodge(0.1), alpha=1)+
  geom_line(position = position_dodge(0.1), alpha=1, size=1)+
  #geom_bar(position="dodge", stat = "identity", size=.3)+
  geom_errorbar(aes(ymin = lowerci, ymax = upperci), width = 0.2, color = "Black", position = position_dodge(0.1)) +
  geom_text(aes(label = round(newAvgSpeed, 2)), size = 6, alpha=1, position = position_dodge(0.6), vjust = -0.5) +
  scale_fill_hue(name="Condition", labels=c("White Cane", "Body-preview aEMA", "Normal aEMA"))+
  ggtitle("Walking Speed per Range and Condition")+
  ylab("Mean walking speed in meters per Second") +
  scale_y_continuous()+
  theme_bw()
```

## Number of detections
The following plot summarizes the number of detections for the baseline and the two conditions at each range. When using the \textit{Regular aEMA} the mean number of detections increased by 39\% between a range of 2-m and 3-m while only increasing 2\% between a range of 3-m and 4-m. Thus, a multiple regression analysis shows that range was a significant predictor when using the \textit{Regular aEMA} ($\beta$ = 0.167, z(177) = 6.18, p < .001), however, range was not significant predictor for obstacle detection with the \textit{Body-preview aEMA} ($\beta$ = 0.033, z(177) = 1.008, p = .3). When comparing the \textit{Body-preview aEMA} to the white cane no significant predictor was found ($\beta$ = 0.04, z(416) = 0.71, p = 0.5), while the \textit{Regular aEMA} detected more obstacles compared to the white cane ($\beta$ = 0.45, z(416) = 8.58, p < .001).

```{r DetectionsPLot, echo=FALSE}
daggDetect <- daggByScen %>%
  group_by(Range, FOD)%>%
  summarise(avgObjDet=mean(objectDetected),
            smean = mean(objectDetected, na.rm = TRUE),
            ssd = sd(objectDetected, na.rm = TRUE),
            count = n()) %>%
  mutate(
    se = ssd / sqrt(count),
    lowerci = lower_ci(smean, se, count),
    upperci = upper_ci(smean, se, count))

ggplot(data = daggDetect, aes(x=Range, y=avgObjDet, group=FOD, color=FOD))+
  geom_point(position = position_dodge(0.1), alpha=1)+
  geom_line(position = position_dodge(0.1), alpha=1, size=1)+
  #geom_bar(position="dodge", stat = "identity", size=.3)+
  geom_errorbar(aes(ymin = lowerci, ymax = upperci), width = 0.2, color = "Black", position = position_dodge(0.1)) +
  geom_text(aes(label = round(avgObjDet, 1)), size = 6, alpha=1, position = position_dodge(0.45), vjust = -0.5) +
  scale_fill_hue(name="Condition", labels=c("White Cane", "Body-preview aEMA", "Normal aEMA"))+
  ggtitle("Number of Objects Detected per Range and Condition")+
  ylab("Mean Obstacles Detected") +
  scale_y_continuous()+
  theme_bw()
```






